nohup: 忽略输入
usage: run_kbert_transformers_cls.py [-h] [--model MODEL]
                                     [--pretrained PRETRAINED] [--cuda]
                                     [--seq_length SEQ_LENGTH]
                                     [--encoder {bert,lstm,gru,cnn,gatedcnn,attn,rcnn,crnn,gpt,bilstm}]
                                     [--bidirectional]
                                     [--pooling {mean,max,first,last}]
                                     [--subword_type {none,char}]
                                     [--sub_vocab_path SUB_VOCAB_PATH]
                                     [--subencoder {avg,lstm,gru,cnn}]
                                     [--sub_layers_num SUB_LAYERS_NUM]
                                     [--tokenizer {bert,char,word,space}]
                                     [--dropout DROPOUT]
                                     [--epochs_num EPOCHS_NUM]
                                     [--batch_size BATCH_SIZE]
                                     [--learning_rate LEARNING_RATE]
                                     [--report_steps REPORT_STEPS] --kg_name
                                     KG_NAME [--workers_num WORKERS_NUM]
                                     [--no_vm] [--no_kg]
run_kbert_transformers_cls.py: error: unrecognized arguments: --no_kg
usage: run_kbert_transformers_cls.py [-h] [--model MODEL]
                                     [--pretrained PRETRAINED] [--cuda]
                                     [--seq_length SEQ_LENGTH]
                                     [--encoder {bert,lstm,gru,cnn,gatedcnn,attn,rcnn,crnn,gpt,bilstm}]
                                     [--bidirectional]
                                     [--pooling {mean,max,first,last}]
                                     [--subword_type {none,char}]
                                     [--sub_vocab_path SUB_VOCAB_PATH]
                                     [--subencoder {avg,lstm,gru,cnn}]
                                     [--sub_layers_num SUB_LAYERS_NUM]
                                     [--tokenizer {bert,char,word,space}]
                                     [--dropout DROPOUT]
                                     [--epochs_num EPOCHS_NUM]
                                     [--batch_size BATCH_SIZE]
                                     [--learning_rate LEARNING_RATE]
                                     [--report_steps REPORT_STEPS] --kg_name
                                     KG_NAME [--workers_num WORKERS_NUM]
                                     [--no_vm] [--no_kg]
run_kbert_transformers_cls.py: error: unrecognized arguments: --no_kg
usage: run_kbert_transformers_cls.py [-h] [--model MODEL]
                                     [--pretrained PRETRAINED] [--cuda]
                                     [--seq_length SEQ_LENGTH]
                                     [--encoder {bert,lstm,gru,cnn,gatedcnn,attn,rcnn,crnn,gpt,bilstm}]
                                     [--bidirectional]
                                     [--pooling {mean,max,first,last}]
                                     [--subword_type {none,char}]
                                     [--sub_vocab_path SUB_VOCAB_PATH]
                                     [--subencoder {avg,lstm,gru,cnn}]
                                     [--sub_layers_num SUB_LAYERS_NUM]
                                     [--tokenizer {bert,char,word,space}]
                                     [--dropout DROPOUT]
                                     [--epochs_num EPOCHS_NUM]
                                     [--batch_size BATCH_SIZE]
                                     [--learning_rate LEARNING_RATE]
                                     [--report_steps REPORT_STEPS] --kg_name
                                     KG_NAME [--workers_num WORKERS_NUM]
                                     [--no_vm] [--no_kg]
run_kbert_transformers_cls.py: error: unrecognized arguments: --no_kg
usage: run_kbert_transformers_cls.py [-h] [--model MODEL]
                                     [--pretrained PRETRAINED] [--cuda]
                                     [--seq_length SEQ_LENGTH]
                                     [--encoder {bert,lstm,gru,cnn,gatedcnn,attn,rcnn,crnn,gpt,bilstm}]
                                     [--bidirectional]
                                     [--pooling {mean,max,first,last}]
                                     [--subword_type {none,char}]
                                     [--sub_vocab_path SUB_VOCAB_PATH]
                                     [--subencoder {avg,lstm,gru,cnn}]
                                     [--sub_layers_num SUB_LAYERS_NUM]
                                     [--tokenizer {bert,char,word,space}]
                                     [--dropout DROPOUT]
                                     [--epochs_num EPOCHS_NUM]
                                     [--batch_size BATCH_SIZE]
                                     [--learning_rate LEARNING_RATE]
                                     [--report_steps REPORT_STEPS] --kg_name
                                     KG_NAME [--workers_num WORKERS_NUM]
                                     [--no_vm] [--no_kg]
run_kbert_transformers_cls.py: error: unrecognized arguments: --cuda
usage: run_kbert_transformers_cls.py [-h] [--model MODEL]
                                     [--pretrained PRETRAINED] [--cuda]
                                     [--seq_length SEQ_LENGTH]
                                     [--encoder {bert,lstm,gru,cnn,gatedcnn,attn,rcnn,crnn,gpt,bilstm}]
                                     [--bidirectional]
                                     [--pooling {mean,max,first,last}]
                                     [--subword_type {none,char}]
                                     [--sub_vocab_path SUB_VOCAB_PATH]
                                     [--subencoder {avg,lstm,gru,cnn}]
                                     [--sub_layers_num SUB_LAYERS_NUM]
                                     [--tokenizer {bert,char,word,space}]
                                     [--dropout DROPOUT]
                                     [--epochs_num EPOCHS_NUM]
                                     [--batch_size BATCH_SIZE]
                                     [--learning_rate LEARNING_RATE]
                                     [--report_steps REPORT_STEPS] --kg_name
                                     KG_NAME [--workers_num WORKERS_NUM]
                                     [--no_vm] [--no_kg]
run_kbert_transformers_cls.py: error: unrecognized arguments: --cuda
  0%|          | 0/82 [00:00<?, ?it/s]100%|██████████| 82/82 [00:00<00:00, 639280.54it/s]
Vocabulary file line 344 has bad format token
Vocabulary Size:  21128
[BertClassifier] use visible_matrix: True
Some weights of the model checkpoint at ./models/bert were not used when initializing BertCNNForMultiLabelSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertCNNForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertCNNForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertCNNForMultiLabelSequenceClassification were not initialized from the model checkpoint at ./models/bert and are newly initialized: ['convs.0.bias', 'convs.1.bias', 'convs.2.weight', 'convs.1.weight', 'convs.2.bias', 'classifier.bias', 'classifier.weight', 'convs.0.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2 GPUs are available. Let's use them.
device:  cuda
Traceback (most recent call last):
  File "/home/lyq2021/experiment/K-BERT-HUGGINGFACE/run_kbert_transformers_cls.py", line 172, in <module>
    main()
  File "/home/lyq2021/experiment/K-BERT-HUGGINGFACE/run_kbert_transformers_cls.py", line 130, in main
    model = model.to(config.device)
  File "/home/lyq2Loading sentences from ./datasets/book_multilabels_task/dev.tsv
There are 2084 sentence in total. We use 1 processes to inject knowledge into sentences.
Progress of process 0: 0/2084
Loading sentences from ./datasets/book_multilabels_task/test.tsv
There are 2067 sentence in total. We use 1 processes to inject knowledge into sentences.
Progress of process 0: 0/2067
Traceback (most recent call last):
  File "/home/lyq2021/experiment/K-BERT-HUGGINGFACE/run_kbert_transformers_cls.py", line 172, in <module>
    main()
  File "/home/lyq2021/experiment/K-BERT-HUGGINGFACE/run_kbert_transformers_cls.py", line 165, in main
    train(model, train_batch, dev_batch, test_batch, config=config, is_MLC=True)
  File "/home/lyq2021/experiment/K-BERT-HUGGINGFACE/train.py", line 52, in train
    loss, _ = model(input_ids=input_ids_batch, 
  File "/home/lyq2021/.conda/envs/lyq2021/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/lyq2021/.conda/envs/lyq2021/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py", line 161, in forward
    inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)
  File "/home/lyq2021/.conda/envs/lyq2021/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py", line 178, in scatter
    return scatter_kwargs(inputs, kwargs, device_ids, dim=self.dim)
  File "/home/lyq2021/.conda/envs/lyq2021/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py", line 53, in scatter_kwargs
    kwargs = scatter(kwargs, target_gpus, dim) if kwargs else []
  File "/home/lyq2021/.conda/envs/lyq2021/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py", line 44, in scatter
    res = scatter_map(inputs)
  File "/home/lyq2021/.conda/envs/lyq2021/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py", line 35, in scatter_map
    return [type(obj)(i) for i in zip(*map(scatter_map, obj.items()))]
  File "/home/lyq2021/.conda/envs/lyq2021/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py", line 31, in scatter_map
    return list(zip(*map(scatter_map, obj)))
  File "/home/lyq2021/.conda/envs/lyq2021/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py", line 27, in scatter_map
    return Scatter.apply(target_gpus, None, dim, obj)
  File "/home/lyq2021/.conda/envs/lyq2021/lib/python3.10/site-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/lyq2021/.conda/envs/lyq2021/lib/python3.10/site-packages/torch/nn/parallel/_functions.py", line 96, in forward
    outputs = comm.scatter(input, target_gpus, chunk_sizes, ctx.dim, streams)
  File "/home/lyq2021/.conda/envs/lyq2021/lib/python3.10/site-packages/torch/nn/parallel/comm.py", line 189, in scatter
    return tuple(torch._C._scatter(tensor, devices, chunk_sizes, dim, streams))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

/site-packages/transformers/models/bert/modeling_bert.py", line 495, in forward
    self_attention_outputs = self.attention(
  File "/home/lyq2021/.conda/envs/lyq2021/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/lyq2021/.conda/envs/lyq2021/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py", line 425, in forward
    self_outputs = self.self(
  File "/home/lyq2021/.conda/envs/lyq2021/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/lyq2021/.conda/envs/lyq2021/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py", line 347, in forward
    attention_scores = attention_scores / math.sqrt(self.attention_head_size)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 10.76 GiB total capacity; 1.02 GiB already allocated; 46.81 MiB free; 1.06 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

